{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Test Data\n",
    "\n",
    "This notebook preprocesses the test data set.  \n",
    "Please change the paths in the second cell to where you store the csv files  \n",
    "\n",
    "There are 2 different notebooks for training data preprocessing and test data preprocessing to ensure our laptop's memory does not get overwhelmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Files and do Preprocessing\n",
    "First we import the necessary datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"C:/Users/hclsa/Desktop/Fall2019/CMPE188/TeamProject/raw_data/test.csv\")\n",
    "building_metadata_preprocessed = pd.read_csv(\"C:/Users/hclsa/Desktop/Fall2019/CMPE188/TeamProject/processed_data/building_metadata_preprocessed.csv\")\n",
    "weather_test_preprocessed = pd.read_csv(\"C:/Users/hclsa/Desktop/Fall2019/CMPE188/TeamProject/processed_data/weather_test_preprocessed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we separate the row from the other values (row will be added back later for the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRow = test['row_id']\n",
    "testRow = pd.DataFrame(testRow, columns = [\"row_id\"])\n",
    "testValues = test.drop(['row_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we combine the testValues, preprocessed building metadata and preprocessed weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_and_building = testValues.merge(building_metadata_preprocessed, on=['building_id'], how='left')\n",
    "test_building_weather = test_and_building.merge(weather_test_preprocessed, on=['site_id', 'timestamp'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we convert the timestamp (date & time) into a single integer reflecting the time (in 24-hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hclsa\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "test_building_weather_time = pd.DataFrame(test_building_weather['timestamp'].apply(lambda x : int(x[10:12])))\n",
    "\n",
    "minMaxScaler = preprocessing.MinMaxScaler()\n",
    "test_building_weather_timeScaled = minMaxScaler.fit_transform(test_building_weather_time)\n",
    "test_building_weather_timeScaled = pd.DataFrame(test_building_weather_timeScaled, columns = [\"hour\"])\n",
    "test_building_weather_time_combined = pd.concat([test_building_weather, test_building_weather_timeScaled], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we converted timestamp into hours, we do not need it anymore and can drop it.  \n",
    "We also drop building ID (as we did for the training set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_building_weather_reduced = test_building_weather_time_combined.drop(['timestamp', 'building_id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weather data has some missing timestamps (times where no weather data was collected)  \n",
    "We impute those missing values with the mean of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanImputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "test_final = meanImputer.fit_transform(test_building_weather_reduced)\n",
    "test_final = pd.DataFrame(test_final, columns = test_building_weather_reduced.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the preprocessed files as CSV\n",
    "Finally, we create two files: 'test_preprocessed.csv' and 'test_row.csv' which can be imported into the Prediction notebook.  \n",
    "We split the preprocessing of training and the preprocessing of testing data to not overload our memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final.to_csv('../CSV/test_preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRow.to_csv('../CSV/test_row.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
